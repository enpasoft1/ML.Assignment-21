# -*- coding: utf-8 -*-
"""logisticregression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/sefahw/machine-learning-assignment/blob/main/logisticregression.ipynb
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

class ScratchLogisticRegression():
    """
    Scratch implementation of logistic regression
    Parameters
    ----------
    num_iter : int
      Number of iterations
    lr : float
      learning rate
    no_bias : bool
      True if no bias term is included.
    verbose : bool
      True if you want to output the learning process.

    Attributes
    ----------
    self.coef_ : The following form of ndarray, shape (n_features,)
      Parameters
    self.loss : The following form of ndarray, shape (self.iter,)
      Record loss to training data
    self.val_loss : The following form of ndarray, shape (self.iter,)
      Record loss against verification data
    """
    
    def __init__(self, num_iter=100, lr=0.01, C=1,
                 random_state=None,bias=False, verbose=False):
        # Record hyperparameters as attributes
        self.iter = num_iter
        self.lr = lr
        self.bias = bias
        self.verbose = verbose
        self.C = C
        self.lamda = 1/self.C
        self.random_state = random_state
        
        # Prepare an array to record the loss
        self.loss = np.zeros(self.iter)
        self.val_loss = np.zeros(self.iter)
    
    def _linear_combination(self,X):
        """
        Join linearly

        Parameters
        ----------
        X : The following form of ndarray, shape (n_samples, n_features)
          Training data

        Returns
        -------
          The following form of ndarray, shape (n_samples, n_features+1)
          linear combination
        """
        x1 = X
        
        # Create x0 (bias term)
        if self.bias == True:
            x0 = np.ones(x1.shape[0])
        else:
            x0 = np.zeros(x1.shape[0])
        
        return np.concatenate([x0.reshape(-1,1),x1],axis=1)
        
        
    def _hypothetical_function(self,X):
        """
        Compute the assumed function of a linear

        Parameters
        ----------
        X : The following form of ndarray, shape (n_samples, n_features)
          Training data

        Returns
        -------
          The following form of ndarray, shape (n_samples, 1)
          Estimation results with linear assumed functions

        """
        x = X
        z = np.dot(x,self.theta)
        
        return 1/(1+np.exp(-z))
    
    def _gradient_descent(self, X, error):
        """
        Using the steepest descent method, the value of θ is updated and learned.
        Parameters
        ----------
        X : The following form of ndarray, shape (n_samples, n_features)
            Features of training data
        error : Error between true value and estimated value
        """
        # tmp to add regularization (lamda) to i=1,2,...
        self.tmp = np.append(0,np.ones(X.shape[1]-1))
        self.theta -= self.lr*(np.dot(error,X) + self.tmp*self.lamda*self.theta)/len(X)
        
        #self.theta -= self.lr*np.dot(error,X)/len(X)
        #self.theta[1:] -= self.lr*self.lamda*self.theta[1:]/len(X)
        
    def _loss_function(self, y, yhat):
        """
        Calculate the loss function for logistic regression.
        Parameters
        ----------
        y : The following form of ndarray, shape (n_samples, )
            Correct answer value of training data
        yhat : The following form of ndarray, shape (n_samples, )
            Correct value of validation data
        """
        
        return np.mean(-y*np.log(yhat) -(1-y)*np.log(1-yhat))+0.5*self.lamda*np.mean(self.theta[1:]**2)
        
    def fit(self, X, y, X_val=False, y_val=False):
        """
        Learn logistic regression.
        When valid data is input, the loss and accuracy for it are also calculated for each iteration.
        When valid data is input, the loss and accuracy for it are also calculated for each iteration.

        Parameters
        ----------
        X : The following form of ndarray, shape (n_samples, n_features)
            Features of training data
        y : The following form of ndarray, shape (n_samples, )
            Correct answer value of training data
        X_val : The following form of ndarray, shape (n_samples, n_features)
            Features of validation data
        y_val : The following form of ndarray, shape (n_samples, )
            Correct value of validation data
        """
        # Get the label value of y
        self.ylabel = np.unique(y)
        
        # Replace the label value of y with 0,1
        y = np.where(y==self.ylabel[0],0,1)
        
        if (type(y_val) != bool):
            y_val = np.where(y_val==self.ylabel[0],0,1)
        
        # linear combination
        X = self._linear_combination(X)
        
        # Give the initial value of parameter θ as a random number.
        np.random.seed(self.random_state)
        self.theta = np.random.rand(X.shape[1])
        
        for i in range(self.iter):
            # Predicted probability
            yhat = self._hypothetical_function(X)
            
            # measurement error
            error = yhat - y            
            self.loss[i] = self._loss_function(y,yhat)
            
            if (type(X_val) != bool):
                val_X = self._linear_combination(X_val)
                val_yhat = self._hypothetical_function(val_X)
                
                self.val_loss[i] = self._loss_function(y_val,val_yhat)
            
            self._gradient_descent(X, error)
            
            # Outputs the learning process when verbose is set to true.
            if self.verbose:
                print('n_iter:', i,
                      'loss:',self.loss[i],
                      'theta:',self.theta)
            
        # Save θ
        np.save('theta', self.theta)
        
            

    def predict(self, X):
        """
        Use logistic regression to estimate the labels.
        Parameters
        ----------
        X : The following form of ndarray, shape (n_samples, n_features)
            Sample

        Returns
        -------
            The following form of ndarray, shape (n_samples, 1)
            Estimation results by logistic regression
        """
        X = self._linear_combination(X)
        yhat = self._hypothetical_function(X)
        
        return np.where(yhat<0.5,self.ylabel[0],self.ylabel[1])
    
    def predict_proba(self, X):
        """
        Use logistic regression to estimate probabilities.
        Parameters
        ----------
        X : The following form of ndarray, shape (n_samples, n_features)
            Sample

        Returns
        -------
            The following form of ndarray, shape (n_samples, 1)
            Estimation results by logistic regression
        """
        X = self._linear_combination(X)
        
        return self._hypothetical_function(X)

x1=np.array([0,1,2,3,4,5,6,7,8])
y=np.array([2,4,2,1,6,7,8,5,2])
x0= np.ones(x1.shape[0])
X=np.concatenate([x0.reshape(-1,1),x1.reshape(-1,1)],axis=1)
print('x:\n',X)

theta=np.random.randn(X.shape[1])
print('theta:\n',theta)

z=np.dot(X,theta)
print('z:\n',z)

y=1/(1+np.exp(-z))
print('y:\n',y)

print

z1=np.arange(-50,50,0.1)
y1=1/(1+np.exp(-z1))

plt .scatter(z1,y1)
plt.show;

X.shape

y.shape

x1=np.array([2,1,5,6,4]).reshape(-1,1)
x0= np.ones(x1.shape[0])
x=np.concatenate([x0.reshape(-1,1),x1.reshape(-1,1)],axis=1)
y=np.array([0.4,0.5,0.2,0.1,0.9])
yhat=np.where(y<0.5,0,1)
y_test=np.array([1,1,0,0,1])
print('x:\n',x)
print('y:\n',y)
print('yhat:\n',yhat)
print('ytest:\n',y_test)

theta=np.random.randn(x.shape[1])
print('theta:\n',theta)

error=yhat-y_test
print('error:\n',error)

lm=np.array([0,1])
theta*lm

0.01*np.dot(error,x)/len(x)+theta*lm/len(x)

theta-=(0.01*np.dot(error,x)/len(x)+theta*lm/len(x))
print ('theta:\n', theta)

theta-=(0.01*np.dot(error,x)/len(x)+theta*lm/len(x))
theta

x1=np.array([2,1,5,6,4]).reshape(-1,1)
x2=np.array([2,1,5,2,4]).reshape(-1,1)
y=np.array([1,1,0,0,1])
print('x1:\n',x1)
print('x2:\n',x2)
print('y:\n',y)

import numpy as np
clf=ScratchLogisticRegression()
clf.fit(x1,y,x1,y)
y_proba=clf.predict_proba(x2)
y_pred=clf.predict(x2)

print('y_proba:\n', y_proba)
print('y_pred:\n',y_pred)

y1=np.ones(99)
y2=np.zeros(99)
yh=np.arange(0.01,1.0,0.01)

j1=-y1*np.log(yh)-(1-y1)*np.log(1-yh)
j2=y2*np.log(yh)-(1-y2)*np.log(1-yh)
j=y1*np.log(yh)-(1-y2)*np.log(1-yh)

fig,ax=plt.subplots(figsize=(8,8))
plt.rcParams["font.size"]=20
plt.scatter(yh,j1,label='if y=1')
plt.scatter(yh,j2,label='if y =0')
plt.scatter(yh,j,label='LOSS')
plt.legend();

from sklearn.datasets import  load_iris
data=load_iris().data
target=load_iris().target.reshape(-1,1)

iris=np.concatenate([data,target],axis=1)
iris=pd.DataFrame(iris)
iris.shape

disp=False
if disp==True:
   sns.pairplot(iris,palette='tab10');

iris_X=iris.loc[iris[4]!=0,2:3].values
iris_y=iris.loc[iris[4]!=0,4].values
print('iris_X.shape:',iris_X.shape)
print('iris_y.shape:',iris_y.shape)

from sklearn.model_selection  import train_test_split
X=iris_X
y=iris_y
X_train,X_valid,y_train,y_valid=\
train_test_split(X,y,train_size=0.8,random_state=0)
print('X_train.shape:',X_train.shape)
print('y_train.shape:',y_train.shape)
print('X_valid.shape',X_valid.shape)
print('y_valid.shape',y_valid.shape)

np.unique(y)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(X_train)
X_train_std= scaler.transform(X_train)
X_valid_std= scaler.transform(X_valid)

clf=ScratchLogisticRegression(num_iter=500,lr=0.01,C=0.01,verbose=False)
clf.fit(X_train_std,y_train,X_valid_std,y_valid)
y_pred=clf.predict(X_valid_std)
y_prob=clf.predict_proba(X_valid_std)

y_valid

y_pred

y_proba

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import  recall_score
from sklearn.metrics import f1_score
accuracy=accuracy_score(y_valid,y_pred)
precision=precision_score(y_valid,y_pred)
recall=recall_score(y_valid,y_pred)
f1=f1_score(y_valid,y_pred)
print('accuracy',accuracy)
print('precision',precision)
print('recall',recall)
print('f1',f1)

from sklearn.linear_model import LogisticRegression
clf2=LogisticRegression()
clf2.fit(X_train_std,y_train)


y_pred2=clf2.predict(X_valid_std)
y_prob2= clf2.predict_proba(X_valid_std)

y_pred2

y_prob

accuracy=accuracy_score(y_valid,y_pred2)
precision=precision_score(y_valid,y_pred2)
recall=recall_score(y_valid,y_pred2)
f1=f1_score(y_valid,y_pred2)
print('accuracy',accuracy)
print('precision',precision)
print('recall',recall)
print('f1',f1)

fig,ax=plt.subplots(figsize=(8,8))
plt.rcParams["font.size"]=20
plt.scatter(range(len(clf.loss)),clf.loss,label='TRAIN')
plt.scatter(range(len(clf.val_loss)),clf.val_loss,label='VALID')
plt.legend();

from matplotlib.colors import ListedColormap
import matplotlib.patches as mpatches

def decision_region(X,y,model,step=0.01,
                    title='decision region',
                    xlabel='xlabel',
                    ylabel='ylabel',
                    target_names=['versicolor', 'virginica']):
    
    # setting
    scatter_color = ['red', 'blue']
    contourf_color = ['pink', 'skyblue']
    n_class = 2
    
    # pred
    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, step), np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, step))
    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]
    y_pred = model.predict(mesh).reshape(mesh_f0.shape)
    
    # plot
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))
    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)
    for i, target in enumerate(set(y)):
        plt.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')
    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]
    plt.legend(handles=patches)
    plt.legend()
    plt.show()

fit,ax=plt.subplots(figsize=(16,9))
decision_region(X_train_std,y_train,clf,title='TRAIN')

fit,ax=plt.subplots(figsize=(16,9))
decision_region(X_valid_std,y_valid,clf,title='VALID')

fit,ax=plt.subplots(figsize=(16,9))
decision_region(X_train_std,y_train,clf2,title='TRAIN')

fit,ax=plt.subplots(figsize=(16,9))
decision_region(X_valid_std,y_valid,clf2,title='VALID')

np.load('theta.npy')